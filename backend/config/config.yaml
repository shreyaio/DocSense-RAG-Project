chunking:
  parent_chunk_size: 512            # tokens
  parent_chunk_overlap: 64
  child_chunk_size: 128
  child_chunk_overlap: 16
  min_chunk_tokens: 40
  separators: ["\n\n", "\n", ". ", " "]

embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  vector_dim: 1024
  query_prefix: "Represent this sentence for searching relevant passages: "
  normalise: true

qdrant:
  mode: "local"                     # "local" | "cloud"
  local_path: "./data/qdrant_store"
  cloud_url: ""                     # fill when upgrading to cloud
  collection_name: "rag_chunks"
  hnsw_m: 16
  hnsw_ef_construct: 100
  hnsw_ef: 64                       # query-time ef â€” higher = more accurate

retrieval:
  dense_top_k: 20
  sparse_top_k: 20
  rrf_k: 60
  rerank_top_k: 20
  final_top_k: 5
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

llm:
  provider: "openrouter"
  model: "mistralai/mistral-7b-instruct:free"
  fallback_model: "google/gemma-3-27b-it:free"
  max_tokens: 1024
  temperature: 0.1
  stream: true
