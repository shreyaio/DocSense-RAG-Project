# ðŸ† RAG SYSTEM â€” SINGLE SOURCE OF TRUTH

> This document is the authoritative reference for the entire project.
> All code generation, architecture decisions, and implementation must strictly follow this document.
> No deviation is allowed unless explicitly approved and this document is updated first.

---

## WHAT WE ARE BUILDING

A production-grade RAG (Retrieval Augmented Generation) web application.
- Phase 1: Single PDF, local storage, single user
- Designed so multi-PDF, multi-user, multi-modal requires zero rewrites â€” only additions
- Every component is behind an abstraction layer so scaling touches config, not logic

---

## FINAL STACK

| Layer | Tool | Notes |
|---|---|---|
| Backend | FastAPI | Async, automatic docs, perfect for AI pipelines |
| Frontend | React + Tailwind | Component-based, deployable to Vercel |
| PDF Parsing | PyMuPDF + pdfplumber | Layout-aware + table detection |
| Chunking | Custom pipeline | RecursiveCharacterTextSplitter + parent-child |
| Embedding | BAAI/bge-large-en-v1.5 | Best free CPU model, MTEB-proven |
| Vector Store | Qdrant local file â†’ Qdrant Cloud (same API) | No Docker, no server |
| Sparse Search | rank-bm25 | Exact match, numbers, rare terms |
| Re-ranker | cross-encoder/ms-marco-MiniLM-L-6-v2 | CPU-friendly, high accuracy |
| LLM | OpenRouter free tier | Model-swappable via config only |
| Async Jobs | FastAPI BackgroundTasks â†’ Celery later | Zero frontend change on upgrade |
| Deployment | Backend â†’ Render, Frontend â†’ Vercel | Both free tier |

---

## FOLDER STRUCTURE

This is the authoritative layout. Every file has exactly one responsibility.
```
rag-system/
â”‚
â”œâ”€â”€ backend/
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                          # HTTP layer only â€” no business logic here
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                   # FastAPI app factory, CORS, middleware, router registration
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ ingest.py             # POST /ingest, GET /ingest/status/{job_id}
â”‚   â”‚       â”œâ”€â”€ query.py              # POST /query
â”‚   â”‚       â””â”€â”€ documents.py          # GET /documents, DELETE /documents/{doc_id}
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                         # Pure business logic â€” zero FastAPI imports
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py          # Orchestrator: parseâ†’chunkâ†’embedâ†’store
â”‚   â”‚   â”‚   â””â”€â”€ retrieval.py          # Orchestrator: searchâ†’rerankâ†’expandâ†’generate
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ parse/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_parser.py         # PyMuPDF pass1 + pdfplumber pass2, outputs ParsedBlock list
â”‚   â”‚   â”‚   â””â”€â”€ structure_detector.py # Heading tree, ToC extraction, header/footer suppression
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ chunk/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py            # Section-boundary split â†’ parent chunks â†’ child chunks
â”‚   â”‚   â”‚   â””â”€â”€ metadata_builder.py   # Builds full ChunkMetadata object per child chunk
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ embed/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ embedder.py           # BGE model loader, batched embed, L2 normalisation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ retrieve/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query_analyser.py     # Detect page refs, section mentions â†’ filter params
â”‚   â”‚   â”‚   â”œâ”€â”€ hybrid_search.py      # Dense + BM25 parallel search + RRF fusion
â”‚   â”‚   â”‚   â”œâ”€â”€ reranker.py           # Cross-encoder scoring, top-k selection
â”‚   â”‚   â”‚   â””â”€â”€ context_builder.py    # Parent fetch, sibling expansion, dedup, assembly
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ generate/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ prompt_builder.py     # Metadata-prefixed context + system prompt construction
â”‚   â”‚       â””â”€â”€ llm_client.py         # OpenRouter API, streaming, model-agnostic
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                      # All storage behind abstract interfaces
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                   # VectorStore ABC, FileStore ABC, BM25Store ABC
â”‚   â”‚   â”œâ”€â”€ qdrant_store.py           # Implements VectorStore â€” local file mode now
â”‚   â”‚   â”œâ”€â”€ bm25_store.py             # Implements BM25Store â€” pickle + corpus JSON
â”‚   â”‚   â””â”€â”€ file_store.py             # Implements FileStore â€” local disk now, S3 interface ready
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Pydantic schemas â€” shared contract across all layers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document.py               # DocumentRecord, IngestionStatus, IngestionJob
â”‚   â”‚   â”œâ”€â”€ chunk.py                  # ParsedBlock, ChildChunk, ParentChunk, ChunkMetadata
â”‚   â”‚   â””â”€â”€ query.py                  # QueryRequest, QueryResponse, Citation, RetrievedContext
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py               # Pydantic BaseSettings â€” reads .env, typed config
â”‚   â”‚   â””â”€â”€ config.yaml               # All tunable parameters â€” single control panel
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_parsing.py
â”‚   â”‚   â”œâ”€â”€ test_chunking.py
â”‚   â”‚   â”œâ”€â”€ test_embedding.py
â”‚   â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â”‚   â””â”€â”€ test_api.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         # Runtime data â€” gitignored entirely
â”‚   â”‚   â”œâ”€â”€ uploads/                  # Raw uploaded PDFs
â”‚   â”‚   â”œâ”€â”€ qdrant_store/             # Qdrant local persistence
â”‚   â”‚   â”œâ”€â”€ bm25_indexes/             # Per-doc BM25 pickles + corpus JSON
â”‚   â”‚   â””â”€â”€ parent_chunks/            # Per-doc parent chunk JSON stores
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ Dockerfile                    # For Render deployment only
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ UploadPanel.jsx        # Drag & drop upload + ingestion progress bar
    â”‚   â”‚   â”œâ”€â”€ DocumentList.jsx       # List indexed docs, delete button per doc
    â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx         # Query input, conversation history, streaming response
    â”‚   â”‚   â”œâ”€â”€ SourcePanel.jsx        # Citation cards: page, section, chunk text preview
    â”‚   â”‚   â””â”€â”€ IngestionStatus.jsx    # Polls /ingest/status/{job_id}, shows % progress
    â”‚   â”‚
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â””â”€â”€ client.js              # All fetch/SSE calls â€” single source for API comms
    â”‚   â”‚
    â”‚   â”œâ”€â”€ App.jsx
    â”‚   â””â”€â”€ main.jsx
    â”‚
    â”œâ”€â”€ package.json
    â””â”€â”€ tailwind.config.js
```

---

## DATA MODELS â€” AUTHORITATIVE CONTRACTS

These field names are locked. Do not rename, remove, or add fields without updating this document.

### models/chunk.py
```python
class ParsedBlock(BaseModel):
    text: str
    page_number: int
    block_type: str                  # "text" | "table" | "caption" | "heading"
    font_size: float | None
    font_name: str | None
    bounding_box: list[float] | None # [x0, y0, x1, y1]
    section_path: str | None         # "Chapter 3 > 3.2 > 3.2.1"
    heading_level: int | None        # 1 | 2 | 3 | None

class ChunkMetadata(BaseModel):
    # Identity
    chunk_id: str                    # sha256(doc_id + str(page_number) + str(char_start))
    parent_id: str
    prev_chunk_id: str | None        # None only for first chunk of document
    next_chunk_id: str | None        # None only for last chunk of document
    # Source location
    doc_id: str
    source_file: str
    page_number: int                 # first page of chunk
    page_range: list[int]            # always two ints: [42, 42] or [42, 43]
    char_start: int
    char_end: int
    # Document structure
    section_title: str | None
    subsection_title: str | None
    heading_level: int | None
    section_path: str | None         # "Chapter 3 > 3.2 Lipid Oxidation"
    # Chunk properties
    block_type: str                  # "text" | "table" | "caption"
    token_count: int                 # child chunk token count
    chunk_index: int                 # absolute position in document
    total_chunks: int                # total child chunks in this document
    is_near_heading: bool            # True if within 2 chunks of a heading block
    chunk_level: str                 # always "child"
    # Indexing info
    embedding_model: str
    created_at: str                  # ISO 8601 UTC

class ChildChunk(BaseModel):
    metadata: ChunkMetadata
    text: str
    embedding: list[float] | None    # None before embedding step

class ParentChunk(BaseModel):
    parent_id: str
    doc_id: str
    text: str
    page_range: list[int]
    section_path: str | None
    child_ids: list[str]
```

### models/document.py
```python
class IngestionStatus(str, Enum):
    pending = "pending"
    processing = "processing"
    completed = "completed"
    failed = "failed"

class IngestionJob(BaseModel):
    job_id: str
    doc_id: str
    status: IngestionStatus
    progress: int                    # 0â€“100
    message: str
    created_at: str
    completed_at: str | None

class DocumentRecord(BaseModel):
    doc_id: str
    filename: str
    file_path: str
    page_count: int
    total_chunks: int
    indexed_at: str
    status: IngestionStatus
    embedding_model: str
```

### models/query.py
```python
class QueryFilters(BaseModel):
    page_range: list[int] | None     # [start_page, end_page]
    section_title: str | None
    block_type: str | None

class QueryRequest(BaseModel):
    question: str
    doc_ids: list[str] | None        # None = search all documents
    top_k: int = 5
    filters: QueryFilters | None

class Citation(BaseModel):
    doc_id: str
    source_file: str
    page_number: int
    page_range: list[int]
    section_path: str | None
    chunk_text_preview: str          # first 200 chars of parent chunk
    relevance_score: float

class RetrievedContext(BaseModel):
    child_chunk_id: str
    parent_text: str
    metadata: ChunkMetadata
    rerank_score: float

class RetrievalStats(BaseModel):
    dense_hits: int
    sparse_hits: int
    fused_candidates: int
    reranked_from: int
    final_count: int

class QueryResponse(BaseModel):
    question: str
    answer: str
    citations: list[Citation]
    model_used: str
    retrieval_stats: RetrievalStats
```

---

## CONFIG.YAML â€” SINGLE CONTROL PANEL
```yaml
chunking:
  parent_chunk_size: 512            # tokens
  parent_chunk_overlap: 64
  child_chunk_size: 128
  child_chunk_overlap: 16
  min_chunk_tokens: 40
  separators: ["\n\n", "\n", ". ", " "]

embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  vector_dim: 1024
  query_prefix: "Represent this sentence for searching relevant passages: "
  normalise: true

qdrant:
  mode: "local"                     # "local" | "cloud"
  local_path: "./data/qdrant_store"
  cloud_url: ""                     # fill when upgrading to cloud
  collection_name: "rag_chunks"
  hnsw_m: 16
  hnsw_ef_construct: 100
  hnsw_ef: 64                       # query-time ef â€” higher = more accurate

retrieval:
  dense_top_k: 20
  sparse_top_k: 20
  rrf_k: 60
  rerank_top_k: 20
  final_top_k: 5

llm:
  provider: "openrouter"
  model: "mistralai/mistral-7b-instruct:free"
  fallback_model: "google/gemma-3-27b-it:free"
  max_tokens: 1024
  temperature: 0.1
  stream: true
```

---

## STORAGE INTERFACES â€” AUTHORITATIVE CONTRACTS
```python
# storage/base.py

class VectorStore(ABC):
    def upsert(self, chunks: list[ChildChunk]) -> None: ...
    def search(self, vector: list[float], top_k: int, filters: dict | None) -> list[SearchResult]: ...
    def delete_document(self, doc_id: str) -> None: ...
    def collection_exists(self) -> bool: ...

class BM25Store(ABC):
    def build(self, doc_id: str, chunks: list[ChildChunk]) -> None: ...
    def search(self, doc_id: str, query: str, top_k: int) -> list[tuple[str, float]]: ...
    def delete(self, doc_id: str) -> None: ...

class FileStore(ABC):
    def save_parent_chunks(self, doc_id: str, parents: list[ParentChunk]) -> None: ...
    def load_parent_chunks(self, doc_id: str) -> dict[str, ParentChunk]: ...
    def save_pdf(self, doc_id: str, file_bytes: bytes) -> str: ...
    def delete_document(self, doc_id: str) -> None: ...
```

Implementations today: `QdrantLocalStore`, `LocalBM25Store`, `LocalFileStore`
Upgrade path: implement the same interface, change one line in settings.

---

## API ENDPOINTS â€” AUTHORITATIVE CONTRACT
```
POST   /ingest
       Body: multipart/form-data { file: PDF }
       Returns: { job_id: str, doc_id: str, message: str }
       Behaviour: saves file, creates IngestionJob, dispatches BackgroundTask, returns immediately

GET    /ingest/status/{job_id}
       Returns: IngestionJob
       Behaviour: frontend polls this to update progress bar

GET    /documents
       Returns: list[DocumentRecord]

DELETE /documents/{doc_id}
       Returns: { success: bool, message: str }
       Behaviour: removes from Qdrant, deletes BM25 index, deletes parent store, deletes uploaded file

POST   /query
       Body: QueryRequest
       Returns: QueryResponse (JSON) or SSE stream if stream=true in config
       Behaviour: if retrieval returns zero results, returns "not found" without calling LLM

GET    /health
       Returns: { status: str, qdrant_ok: bool, models_loaded: bool }
```

---

## FULL PIPELINE FLOWS

### Ingestion Pipeline
```
POST /ingest (PDF binary)
  â””â”€ [api/routes/ingest.py]
       Validate file type + size
       Generate doc_id = sha256(filename + timestamp)
       Save PDF via FileStore.save_pdf()
       Create IngestionJob (status: pending)
       Dispatch to BackgroundTasks â†’ core/pipeline/ingestion.py
       Return { job_id, doc_id } immediately

  â””â”€ [core/pipeline/ingestion.py]  â† runs in background
       progress=5%  â†’ status=processing

  â””â”€ [core/parse/pdf_parser.py]
       Pass 1 â€” PyMuPDF:
         Per page: extract text blocks with bbox, font_size, font_name
         Flag font_size > median â†’ heading candidate
         Detect repeated blocks at same Y across 3+ pages â†’ suppress as header/footer
       Pass 2 â€” pdfplumber:
         Detect table bounding boxes per page
         Blocks overlapping table bbox â†’ block_type=table, extract as markdown
       Output: list[ParsedBlock]
       progress=25%

  â””â”€ [core/parse/structure_detector.py]
       Try fitz.get_toc() â†’ use native ToC if available
       Otherwise infer H1/H2/H3 from relative font sizes
       Build section tree
       Assign section_path to every ParsedBlock
       Output: enriched list[ParsedBlock] + section_map dict
       progress=35%

  â””â”€ [core/chunk/chunker.py]
       For each section boundary group:
         Apply RecursiveCharacterTextSplitter â†’ parent chunks (512 tok, 64 overlap)
         For each parent: subdivide â†’ child chunks (128 tok, 16 overlap)
         Tables â†’ single atomic child + parent, never split
         Short blocks < 40 tokens â†’ merge with next sibling
         Assign prev_chunk_id / next_chunk_id across all children
       Output: list[ChildChunk], list[ParentChunk]
       progress=50%

  â””â”€ [core/chunk/metadata_builder.py]
       For each child chunk:
         chunk_id = sha256(doc_id + str(page_number) + str(char_start))
         Populate all ChunkMetadata fields
         Compute is_near_heading
       Output: list[ChildChunk with complete metadata]
       progress=55%

  â””â”€ [core/embed/embedder.py]
       Load BAAI/bge-large-en-v1.5 (cached after first load)
       Embed child chunk texts in batches of 32
       L2-normalise each embedding vector
       Attach embeddings to ChildChunk objects
       Output: list[ChildChunk with embeddings]
       progress=80%

  â””â”€ [storage/qdrant_store.py]
       Create collection if not exists (dim=1024, cosine, HNSW config from config.yaml)
       Create payload indexes: page_number, section_title, block_type, doc_id
       Upsert all child chunks: vector + full metadata payload

  â””â”€ [storage/bm25_store.py]
       Build BM25Okapi corpus from child chunk texts in chunk_index order
       Save {corpus_index: chunk_id} mapping as JSON
       Pickle BM25 object to ./data/bm25_indexes/{doc_id}.pkl

  â””â”€ [storage/file_store.py]
       Serialise ParentChunk list â†’ ./data/parent_chunks/{doc_id}.json

       progress=100% â†’ status=completed
       On any error: status=failed, message=error details
```

### Retrieval Pipeline
```
POST /query (QueryRequest)
  â””â”€ [api/routes/query.py]
       Validate QueryRequest
       Call core/pipeline/retrieval.py
       Return QueryResponse

  â””â”€ [core/pipeline/retrieval.py]  â† orchestrator

  â””â”€ [core/retrieve/query_analyser.py]
       Scan question for:
         "page X" / "page X to Y" â†’ page_range filter
         Section name fuzzy match against section_map â†’ section_title filter
         doc_ids from request â†’ always applied if provided
       Output: filter_params dict (may be empty)

  â””â”€ [core/retrieve/hybrid_search.py]  â† dense + sparse run independently

       DENSE ARM:
         Prepend BGE query prefix to question text
         Embed with BGE model (same cached instance)
         L2-normalise query vector
         Qdrant search: top dense_top_k, apply filter_params
         Returns: list[(chunk_id, cosine_score, metadata)]

       SPARSE ARM:
         Load BM25 index for each requested doc_id
         Tokenise query
         BM25.get_scores() â†’ top sparse_top_k via corpus_indexâ†’chunk_id map
         Returns: list[(chunk_id, bm25_score)]

       RRF FUSION:
         rrf_score = 1/(rrf_k + rank_dense) + 1/(rrf_k + rank_sparse)
         Chunks in one arm only get 0 for the missing arm rank
         Sort by rrf_score descending â†’ top 20 fused candidates

  â””â”€ [core/retrieve/reranker.py]
       Load cross-encoder/ms-marco-MiniLM-L-6-v2 (cached)
       Score each (question, child_chunk_text) pair â€” batched, CPU
       Sort by score â†’ keep top final_top_k
       Output: list[RetrievedContext] with rerank_score

  â””â”€ [core/retrieve/context_builder.py]
       For each of top 5 re-ranked child chunks:
         Fetch parent from FileStore.load_parent_chunks()
         If chunk is near section boundary: also fetch sibling child text
         Deduplicate: two children sharing parent_id â†’ include parent once
       Output: list[RetrievedContext] â€” max 5 unique parents with metadata

       If output is empty â†’ return QueryResponse with answer="not found", skip LLM

  â””â”€ [core/generate/prompt_builder.py]
       Build context string with metadata headers:
         "[SOURCE: {source_file} | Page {page_range} | {section_path}]
          {parent_text}"
       Build system prompt:
         "You are a document-grounded assistant.
          Rules: answer only from context, cite page numbers and sections,
          say 'not found in document' if absent, do not speculate."
       Output: messages list for OpenRouter API

  â””â”€ [core/generate/llm_client.py]
       POST to OpenRouter /v1/chat/completions
       Model from config.yaml with fallback_model on failure
       Stream tokens via SSE if config.stream=true
       Output: answer string

  â””â”€ [api/routes/query.py]
       Assemble QueryResponse:
         answer, citations, model_used, retrieval_stats, question
       Return to frontend
```

---

## FRONTEND LAYOUT
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEFT           â”‚  CENTRE                  â”‚  RIGHT           â”‚
â”‚  Documents      â”‚  Chat                    â”‚  Sources         â”‚
â”‚                 â”‚                          â”‚                  â”‚
â”‚ [Upload PDF]    â”‚  Q: What are macros?     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                 â”‚                          â”‚ â”‚ Page 42      â”‚ â”‚
â”‚ âœ“ nutrition.pdf â”‚  A: Macronutrients are   â”‚ â”‚ Chapter 3    â”‚ â”‚
â”‚   3241 chunks   â”‚  the three main... â–‹     â”‚ â”‚ "The three   â”‚ â”‚
â”‚   [Delete]      â”‚  (streaming)             â”‚ â”‚ macros are.."â”‚ â”‚
â”‚                 â”‚                          â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ [+ Upload New]  â”‚  [Type question here...] â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                 â”‚  [Send]                  â”‚ â”‚ Page 87      â”‚ â”‚
â”‚                 â”‚                          â”‚ â”‚ Chapter 5    â”‚ â”‚
â”‚                 â”‚                          â”‚ â”‚ "Vitamins.." â”‚ â”‚
â”‚                 â”‚                          â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SCALING PATH

| Stage | What Changes | What Stays the Same |
|---|---|---|
| Multi-PDF search | Remove doc_id filter in query | Everything else |
| Qdrant Cloud | config.yaml: mode=cloud, fill cloud_url | All pipeline logic |
| S3 File Storage | Implement S3FileStore(FileStore) | All pipeline logic |
| Per-user isolation | Add user_id to metadata + Qdrant filter | All pipeline logic |
| Async at scale | Swap BackgroundTasks for Celery | All pipeline logic, all API contracts |
| Multi-modal | Add image_extractor.py, block_type=image | Existing text pipeline untouched |
| Auth | Add FastAPI-Users middleware | All pipeline logic |

---

## IMPLEMENTATION ORDER

Build and test each step independently before proceeding.
```
Step 1  â†’ models/chunk.py, models/document.py, models/query.py
Step 2  â†’ config/settings.py + config/config.yaml
Step 3  â†’ core/parse/pdf_parser.py          [test: print ParsedBlocks from real PDF]
Step 4  â†’ core/parse/structure_detector.py  [test: verify section_path on every block]
Step 5  â†’ core/chunk/chunker.py             [test: chunk counts, no cross-section chunks, table atomicity]
Step 6  â†’ core/chunk/metadata_builder.py    [test: deterministic IDs, prev/next chain integrity]
Step 7  â†’ core/embed/embedder.py            [test: embed 10 chunks, verify normalisation]
Step 8  â†’ storage/base.py                   [interfaces only â€” no implementation]
Step 9  â†’ storage/qdrant_store.py           [test: upsert + search + filter on test data]
Step 10 â†’ storage/bm25_store.py             [test: build + keyword query + corpus mapping]
Step 11 â†’ storage/file_store.py             [test: save + load parent chunks]
Step 12 â†’ core/pipeline/ingestion.py        [test: full end-to-end on real PDF, verify all stores]
Step 13 â†’ core/retrieve/query_analyser.py   [test: filter extraction on sample queries]
Step 14 â†’ core/retrieve/hybrid_search.py    [test: dense + sparse + RRF, print ranked results]
Step 15 â†’ core/retrieve/reranker.py         [test: verify reranked order differs from RRF order]
Step 16 â†’ core/retrieve/context_builder.py  [test: parent fetch, sibling expansion, dedup]
Step 17 â†’ core/generate/prompt_builder.py   [test: print assembled prompt, verify metadata headers]
Step 18 â†’ core/generate/llm_client.py       [test: real OpenRouter call with assembled prompt]
Step 19 â†’ core/pipeline/retrieval.py        [test: full end-to-end question â†’ cited answer]
Step 20 â†’ api/main.py + api/routes/         [test: curl/Postman all endpoints]
Step 21 â†’ frontend/                         [connect to API, test uploadâ†’queryâ†’sources flow]
Step 22 â†’ Dockerfile + Render + Vercel deploy
```

---

## EXECUTION RULES

### General Principles

1. Separation of concerns is mandatory.
2. API layer contains HTTP handling only â€” no business logic.
3. core/ contains pure business logic â€” zero FastAPI imports.
4. storage/ implements abstract interfaces only â€” no pipeline logic.
5. models/ defines contracts. Never modified casually.
6. config.yaml is the only place for tunable parameters.
7. No hidden global state.
8. No hardcoded values anywhere â€” model names, paths, keys, sizes.
9. Every component must be independently testable.
10. Architecture clarity over clever shortcuts.
11. Storage layer is the only code that knows about persistence details.
12. Pipeline orchestrators call modules in sequence â€” they contain no logic themselves.
13. Each pipeline step receives its input explicitly â€” never reads from disk/DB directly.

### DO's
```
âœ” Follow the exact folder structure defined above.
âœ” Implement one file at a time.
âœ” Match Pydantic model field names exactly as defined in this document.
âœ” Use type hints everywhere.
âœ” Keep functions small and single-purpose.
âœ” Load all configuration via config/settings.py only.
âœ” Use dependency injection â€” pass dependencies explicitly.
âœ” Keep VectorStore, BM25Store, and FileStore behind their interfaces.
âœ” Normalise embeddings if config.embedding.normalise = true.
âœ” Log important pipeline steps: ingestion progress, retrieval stats.
âœ” Ensure deterministic chunk_id generation.
âœ” Ensure parent-child chunk linkage integrity before storage.
âœ” Ensure all metadata fields are populated before embedding step.
âœ” Use async only at the API layer â€” core/ is synchronous.
âœ” Write minimal but clear docstrings per function.
âœ” Return job_id immediately from /ingest â€” never block the HTTP response.
âœ” Store job status in-memory (or Redis later) â€” never poll the vector DB for job state.
âœ” Build BM25 index per doc_id, load per doc_id â€” never globally merged.
âœ” Store parent chunk text only in FileStore â€” never in Qdrant payload.
âœ” Always include retrieval_stats in QueryResponse for debuggability.
âœ” Support both streaming and non-streaming in llm_client.py.
âœ” Construct all file paths via FileStore â€” never with scattered os.path calls.
âœ” You MAY import from models/, config/, and storage/base.py â€” these are stable contracts.
```

### DON'Ts
```
âœ˜ Do NOT merge layers â€” no embedding inside API routes.
âœ˜ Do NOT import FastAPI inside core/.
âœ˜ Do NOT import storage implementations directly inside pipeline logic â€” use interfaces.
âœ˜ Do NOT hardcode model names, chunk sizes, paths, or API keys anywhere.
âœ˜ Do NOT bypass abstraction layers.
âœ˜ Do NOT introduce LangChain, LlamaIndex, or other orchestration frameworks.
âœ˜ Do NOT auto-generate architecture changes â€” follow this document.
âœ˜ Do NOT rename schema fields without updating this entire document first.
âœ˜ Do NOT collapse parent-child chunking into a single level.
âœ˜ Do NOT remove metadata fields.
âœ˜ Do NOT store parent chunk text inside Qdrant payload â€” FileStore only.
âœ˜ Do NOT load the full BM25 corpus for all documents when querying one document.
âœ˜ Do NOT run reranker before RRF fusion is complete.
âœ˜ Do NOT send child chunk text to the LLM â€” always send parent chunk text.
âœ˜ Do NOT call the LLM if retrieval returns zero results â€” return "not found" directly.
âœ˜ Do NOT silently fail on ingestion errors â€” update job to status=failed with message.
âœ˜ Do NOT create unnecessary helper abstractions.
âœ˜ Do NOT optimise prematurely.
âœ˜ Do NOT introduce unused features.
```

### Metadata Rules
```
- Every child chunk must have all ChunkMetadata fields populated before storage.
- chunk_id must be deterministic: sha256(doc_id + str(page_number) + str(char_start)).
- prev_chunk_id and next_chunk_id must be set â€” None only for first/last chunk of document.
- section_path must be human-readable: "Chapter 3 > 3.2 Lipid Oxidation".
- is_near_heading must be computed before metadata is finalised.
- page_range is always a two-int list, even single-page chunks: [42, 42].
- token_count reflects child chunk text length â€” not parent.
- chunk_level is always explicitly set â€” never inferred.
```

### Ingestion Pipeline Rules
```
Sequence: parse â†’ structure_detect â†’ chunk â†’ build_metadata â†’ embed â†’ store_vectors + store_bm25 + store_parents

- Parsing must be separate from chunking.
- Chunking must be separate from embedding.
- Embedding must be separate from storage.
- Metadata must be built before embedding.
- Storage must only accept fully-formed ChildChunk objects.
- No implicit state sharing between steps.
- Progress updates must reflect actual stage completion.
- On any error: catch, set job status=failed, set message=error details, stop pipeline.
```

### Retrieval Pipeline Rules
```
Sequence: analyse_query â†’ hybrid_search (dense âˆ¥ sparse) â†’ RRF fusion â†’ rerank â†’ build_context â†’ build_prompt â†’ call_llm â†’ return_response

- Query analysis must not perform retrieval.
- Dense and sparse search must run independently before fusion.
- Fusion must occur after both arms complete.
- Reranking must operate only on fused results.
- Context builder must fetch parents via FileStore abstraction only.
- Prompt builder must not call storage directly.
- LLM client must not know about retrieval internals.
- If context_builder returns empty list: skip LLM, return "not found in document".
```

### Config Rules
```
- All model names must come from config.yaml.
- All chunk sizes must come from config.yaml.
- All retrieval top-k values must come from config.yaml.
- Switching from local to cloud Qdrant requires only a config change.
- No logic must break if config values are adjusted within reasonable ranges.
```

### Testing Rules
```
- Each module must be testable in isolation.
- No API server required to test core logic.
- Do not load the embedding model inside tests unless explicitly testing embedder.py.
- Use small synthetic data for retrieval and chunking tests.
- Test deterministic chunk_id generation with identical inputs.
- Test that ingestion failure updates job status correctly.
```

### Deployment Rules
```
- Secrets (OpenRouter key, Qdrant cloud key) via environment variables only.
- Non-secret tunable parameters via config.yaml only.
- .env.example must be kept current with all required variable names.
- Backend Dockerfile must not embed secrets.
- Frontend reads API base URL from environment variable â€” never hardcoded.
- CORS configured in api/main.py only â€” never in individual routes.
```

### AI Code Generation Rules
```
- Generate one file at a time.
- Do not assume other files exist unless defined in this document.
- Do not invent new architecture or new abstractions.
- Do not refactor the folder structure.
- Do not simplify the pipeline unless explicitly instructed.
- Ask for clarification instead of guessing.
- You MAY import from models/, config/, and storage/base.py â€” these are stable.
- Reference this document as ground truth for all field names, sequences, and interfaces.
```

---

## GOAL

Build a production-grade, scalable, modular RAG system that is architecturally clean,
extensible, and defensible in any technical interview or code review.

**Execution over improvisation. Clarity over cleverness. Structure over shortcuts.**